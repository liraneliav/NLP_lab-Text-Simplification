{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 3330,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03003003003003003,
      "grad_norm": 0.31671950221061707,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0281,
      "step": 10
    },
    {
      "epoch": 0.06006006006006006,
      "grad_norm": 0.17749135196208954,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0143,
      "step": 20
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 0.32796624302864075,
      "learning_rate": 3e-06,
      "loss": 0.0212,
      "step": 30
    },
    {
      "epoch": 0.12012012012012012,
      "grad_norm": 0.19683758914470673,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0163,
      "step": 40
    },
    {
      "epoch": 0.15015015015015015,
      "grad_norm": 0.14618273079395294,
      "learning_rate": 5e-06,
      "loss": 0.0156,
      "step": 50
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 0.12360560894012451,
      "learning_rate": 6e-06,
      "loss": 0.0138,
      "step": 60
    },
    {
      "epoch": 0.21021021021021022,
      "grad_norm": 0.15949398279190063,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0118,
      "step": 70
    },
    {
      "epoch": 0.24024024024024024,
      "grad_norm": 0.06836540997028351,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0164,
      "step": 80
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.13675075769424438,
      "learning_rate": 9e-06,
      "loss": 0.011,
      "step": 90
    },
    {
      "epoch": 0.3003003003003003,
      "grad_norm": 0.15220417082309723,
      "learning_rate": 1e-05,
      "loss": 0.009,
      "step": 100
    },
    {
      "epoch": 0.3303303303303303,
      "grad_norm": 0.14548632502555847,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0118,
      "step": 110
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 0.11972995847463608,
      "learning_rate": 1.2e-05,
      "loss": 0.0104,
      "step": 120
    },
    {
      "epoch": 0.39039039039039036,
      "grad_norm": 0.11690103262662888,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.008,
      "step": 130
    },
    {
      "epoch": 0.42042042042042044,
      "grad_norm": 0.07128453999757767,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0091,
      "step": 140
    },
    {
      "epoch": 0.45045045045045046,
      "grad_norm": 0.09011898934841156,
      "learning_rate": 1.5e-05,
      "loss": 0.0091,
      "step": 150
    },
    {
      "epoch": 0.4804804804804805,
      "grad_norm": 0.07009632885456085,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0077,
      "step": 160
    },
    {
      "epoch": 0.5105105105105106,
      "grad_norm": 0.05591296777129173,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0067,
      "step": 170
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 0.060909781605005264,
      "learning_rate": 1.8e-05,
      "loss": 0.0069,
      "step": 180
    },
    {
      "epoch": 0.5705705705705706,
      "grad_norm": 0.06024838611483574,
      "learning_rate": 1.9e-05,
      "loss": 0.0074,
      "step": 190
    },
    {
      "epoch": 0.6006006006006006,
      "grad_norm": 0.04834023118019104,
      "learning_rate": 2e-05,
      "loss": 0.0069,
      "step": 200
    },
    {
      "epoch": 0.6306306306306306,
      "grad_norm": 0.05477294325828552,
      "learning_rate": 2.1e-05,
      "loss": 0.0074,
      "step": 210
    },
    {
      "epoch": 0.6606606606606606,
      "grad_norm": 0.08351447433233261,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0063,
      "step": 220
    },
    {
      "epoch": 0.6906906906906907,
      "grad_norm": 0.062457989901304245,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0051,
      "step": 230
    },
    {
      "epoch": 0.7207207207207207,
      "grad_norm": 0.06822056323289871,
      "learning_rate": 2.4e-05,
      "loss": 0.0061,
      "step": 240
    },
    {
      "epoch": 0.7507507507507507,
      "grad_norm": 0.028943631798028946,
      "learning_rate": 2.5e-05,
      "loss": 0.0047,
      "step": 250
    },
    {
      "epoch": 0.7807807807807807,
      "grad_norm": 0.03850378468632698,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0048,
      "step": 260
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 0.03694329038262367,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0053,
      "step": 270
    },
    {
      "epoch": 0.8408408408408409,
      "grad_norm": 0.05021940916776657,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0045,
      "step": 280
    },
    {
      "epoch": 0.8708708708708709,
      "grad_norm": 0.043645501136779785,
      "learning_rate": 2.9e-05,
      "loss": 0.0039,
      "step": 290
    },
    {
      "epoch": 0.9009009009009009,
      "grad_norm": 0.09162106364965439,
      "learning_rate": 3e-05,
      "loss": 0.0044,
      "step": 300
    },
    {
      "epoch": 0.9309309309309309,
      "grad_norm": 0.04006672278046608,
      "learning_rate": 3.1e-05,
      "loss": 0.0046,
      "step": 310
    },
    {
      "epoch": 0.960960960960961,
      "grad_norm": 0.06439992785453796,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0044,
      "step": 320
    },
    {
      "epoch": 0.990990990990991,
      "grad_norm": 0.035129524767398834,
      "learning_rate": 3.3e-05,
      "loss": 0.0036,
      "step": 330
    },
    {
      "epoch": 1.021021021021021,
      "grad_norm": 0.03983226791024208,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0034,
      "step": 340
    },
    {
      "epoch": 1.0510510510510511,
      "grad_norm": 0.05232676863670349,
      "learning_rate": 3.5e-05,
      "loss": 0.0046,
      "step": 350
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 0.057288095355033875,
      "learning_rate": 3.6e-05,
      "loss": 0.0041,
      "step": 360
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.039172589778900146,
      "learning_rate": 3.7e-05,
      "loss": 0.004,
      "step": 370
    },
    {
      "epoch": 1.1411411411411412,
      "grad_norm": 0.04855107143521309,
      "learning_rate": 3.8e-05,
      "loss": 0.0036,
      "step": 380
    },
    {
      "epoch": 1.1711711711711712,
      "grad_norm": 0.0248482134193182,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0037,
      "step": 390
    },
    {
      "epoch": 1.2012012012012012,
      "grad_norm": 0.04653419926762581,
      "learning_rate": 4e-05,
      "loss": 0.0037,
      "step": 400
    },
    {
      "epoch": 1.2312312312312312,
      "grad_norm": 0.09325579553842545,
      "learning_rate": 4.1e-05,
      "loss": 0.003,
      "step": 410
    },
    {
      "epoch": 1.2612612612612613,
      "grad_norm": 0.03432203456759453,
      "learning_rate": 4.2e-05,
      "loss": 0.0033,
      "step": 420
    },
    {
      "epoch": 1.2912912912912913,
      "grad_norm": 0.04539383947849274,
      "learning_rate": 4.3e-05,
      "loss": 0.0034,
      "step": 430
    },
    {
      "epoch": 1.3213213213213213,
      "grad_norm": 0.029159240424633026,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.003,
      "step": 440
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 0.03810841590166092,
      "learning_rate": 4.5e-05,
      "loss": 0.0033,
      "step": 450
    },
    {
      "epoch": 1.3813813813813813,
      "grad_norm": 0.050381600856781006,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0026,
      "step": 460
    },
    {
      "epoch": 1.4114114114114114,
      "grad_norm": 0.029561080038547516,
      "learning_rate": 4.7e-05,
      "loss": 0.0037,
      "step": 470
    },
    {
      "epoch": 1.4414414414414414,
      "grad_norm": 0.023290462791919708,
      "learning_rate": 4.8e-05,
      "loss": 0.0026,
      "step": 480
    },
    {
      "epoch": 1.4714714714714714,
      "grad_norm": 0.020813211798667908,
      "learning_rate": 4.9e-05,
      "loss": 0.0029,
      "step": 490
    },
    {
      "epoch": 1.5015015015015014,
      "grad_norm": 0.021200455725193024,
      "learning_rate": 5e-05,
      "loss": 0.0026,
      "step": 500
    },
    {
      "epoch": 1.5315315315315314,
      "grad_norm": 0.05468630790710449,
      "learning_rate": 4.982332155477032e-05,
      "loss": 0.0027,
      "step": 510
    },
    {
      "epoch": 1.5615615615615615,
      "grad_norm": 0.027263330295681953,
      "learning_rate": 4.9646643109540636e-05,
      "loss": 0.0028,
      "step": 520
    },
    {
      "epoch": 1.5915915915915915,
      "grad_norm": 0.026665564626455307,
      "learning_rate": 4.946996466431096e-05,
      "loss": 0.0029,
      "step": 530
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 0.015036111697554588,
      "learning_rate": 4.929328621908128e-05,
      "loss": 0.0024,
      "step": 540
    },
    {
      "epoch": 1.6516516516516515,
      "grad_norm": 0.022172970697283745,
      "learning_rate": 4.9116607773851593e-05,
      "loss": 0.0027,
      "step": 550
    },
    {
      "epoch": 1.6816816816816815,
      "grad_norm": 0.04732317104935646,
      "learning_rate": 4.893992932862191e-05,
      "loss": 0.0021,
      "step": 560
    },
    {
      "epoch": 1.7117117117117115,
      "grad_norm": 0.017906665802001953,
      "learning_rate": 4.8763250883392234e-05,
      "loss": 0.0022,
      "step": 570
    },
    {
      "epoch": 1.7417417417417418,
      "grad_norm": 0.0426998995244503,
      "learning_rate": 4.858657243816255e-05,
      "loss": 0.0033,
      "step": 580
    },
    {
      "epoch": 1.7717717717717718,
      "grad_norm": 0.03150585666298866,
      "learning_rate": 4.840989399293286e-05,
      "loss": 0.003,
      "step": 590
    },
    {
      "epoch": 1.8018018018018018,
      "grad_norm": 0.031798992305994034,
      "learning_rate": 4.823321554770318e-05,
      "loss": 0.0024,
      "step": 600
    },
    {
      "epoch": 1.8318318318318318,
      "grad_norm": 0.03243635967373848,
      "learning_rate": 4.8056537102473495e-05,
      "loss": 0.0021,
      "step": 610
    },
    {
      "epoch": 1.8618618618618619,
      "grad_norm": 0.022263294085860252,
      "learning_rate": 4.787985865724382e-05,
      "loss": 0.0026,
      "step": 620
    },
    {
      "epoch": 1.8918918918918919,
      "grad_norm": 0.027598518878221512,
      "learning_rate": 4.7703180212014135e-05,
      "loss": 0.0024,
      "step": 630
    },
    {
      "epoch": 1.921921921921922,
      "grad_norm": 0.03696730360388756,
      "learning_rate": 4.752650176678445e-05,
      "loss": 0.0023,
      "step": 640
    },
    {
      "epoch": 1.951951951951952,
      "grad_norm": 0.02454782836139202,
      "learning_rate": 4.734982332155477e-05,
      "loss": 0.0023,
      "step": 650
    },
    {
      "epoch": 1.981981981981982,
      "grad_norm": 0.04492334648966789,
      "learning_rate": 4.717314487632509e-05,
      "loss": 0.0027,
      "step": 660
    },
    {
      "epoch": 2.012012012012012,
      "grad_norm": 0.02186453901231289,
      "learning_rate": 4.699646643109541e-05,
      "loss": 0.0025,
      "step": 670
    },
    {
      "epoch": 2.042042042042042,
      "grad_norm": 0.027308020740747452,
      "learning_rate": 4.6819787985865726e-05,
      "loss": 0.0021,
      "step": 680
    },
    {
      "epoch": 2.0720720720720722,
      "grad_norm": 0.02123684622347355,
      "learning_rate": 4.664310954063604e-05,
      "loss": 0.0025,
      "step": 690
    },
    {
      "epoch": 2.1021021021021022,
      "grad_norm": 0.08408579230308533,
      "learning_rate": 4.646643109540637e-05,
      "loss": 0.0021,
      "step": 700
    },
    {
      "epoch": 2.1321321321321323,
      "grad_norm": 0.03901873528957367,
      "learning_rate": 4.6289752650176684e-05,
      "loss": 0.0024,
      "step": 710
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 0.02933633327484131,
      "learning_rate": 4.6113074204947e-05,
      "loss": 0.0022,
      "step": 720
    },
    {
      "epoch": 2.1921921921921923,
      "grad_norm": 0.030472995713353157,
      "learning_rate": 4.593639575971732e-05,
      "loss": 0.0026,
      "step": 730
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.0355398915708065,
      "learning_rate": 4.5759717314487634e-05,
      "loss": 0.0028,
      "step": 740
    },
    {
      "epoch": 2.2522522522522523,
      "grad_norm": 0.026691703125834465,
      "learning_rate": 4.558303886925796e-05,
      "loss": 0.0027,
      "step": 750
    },
    {
      "epoch": 2.2822822822822824,
      "grad_norm": 0.021647654473781586,
      "learning_rate": 4.5406360424028275e-05,
      "loss": 0.0022,
      "step": 760
    },
    {
      "epoch": 2.3123123123123124,
      "grad_norm": 0.02109614387154579,
      "learning_rate": 4.5229681978798585e-05,
      "loss": 0.0018,
      "step": 770
    },
    {
      "epoch": 2.3423423423423424,
      "grad_norm": 0.01831749454140663,
      "learning_rate": 4.50530035335689e-05,
      "loss": 0.0025,
      "step": 780
    },
    {
      "epoch": 2.3723723723723724,
      "grad_norm": 0.06301944702863693,
      "learning_rate": 4.4876325088339225e-05,
      "loss": 0.0019,
      "step": 790
    },
    {
      "epoch": 2.4024024024024024,
      "grad_norm": 0.022611605003476143,
      "learning_rate": 4.469964664310954e-05,
      "loss": 0.0019,
      "step": 800
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 0.028586316853761673,
      "learning_rate": 4.452296819787986e-05,
      "loss": 0.0022,
      "step": 810
    },
    {
      "epoch": 2.4624624624624625,
      "grad_norm": 0.1526268869638443,
      "learning_rate": 4.4346289752650176e-05,
      "loss": 0.0023,
      "step": 820
    },
    {
      "epoch": 2.4924924924924925,
      "grad_norm": 0.022032005712389946,
      "learning_rate": 4.416961130742049e-05,
      "loss": 0.0022,
      "step": 830
    },
    {
      "epoch": 2.5225225225225225,
      "grad_norm": 0.02593761309981346,
      "learning_rate": 4.3992932862190816e-05,
      "loss": 0.0025,
      "step": 840
    },
    {
      "epoch": 2.5525525525525525,
      "grad_norm": 0.03543698415160179,
      "learning_rate": 4.381625441696113e-05,
      "loss": 0.0028,
      "step": 850
    },
    {
      "epoch": 2.5825825825825826,
      "grad_norm": 0.04096956178545952,
      "learning_rate": 4.363957597173145e-05,
      "loss": 0.0021,
      "step": 860
    },
    {
      "epoch": 2.6126126126126126,
      "grad_norm": 0.05321818217635155,
      "learning_rate": 4.346289752650177e-05,
      "loss": 0.0024,
      "step": 870
    },
    {
      "epoch": 2.6426426426426426,
      "grad_norm": 0.04107781499624252,
      "learning_rate": 4.328621908127209e-05,
      "loss": 0.0025,
      "step": 880
    },
    {
      "epoch": 2.6726726726726726,
      "grad_norm": 0.03473815321922302,
      "learning_rate": 4.310954063604241e-05,
      "loss": 0.0026,
      "step": 890
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 0.03127769008278847,
      "learning_rate": 4.2932862190812724e-05,
      "loss": 0.0023,
      "step": 900
    },
    {
      "epoch": 2.7327327327327327,
      "grad_norm": 0.02863815426826477,
      "learning_rate": 4.275618374558304e-05,
      "loss": 0.0026,
      "step": 910
    },
    {
      "epoch": 2.7627627627627627,
      "grad_norm": 0.02485155500471592,
      "learning_rate": 4.257950530035336e-05,
      "loss": 0.0022,
      "step": 920
    },
    {
      "epoch": 2.7927927927927927,
      "grad_norm": 0.02860897406935692,
      "learning_rate": 4.240282685512368e-05,
      "loss": 0.0027,
      "step": 930
    },
    {
      "epoch": 2.8228228228228227,
      "grad_norm": 0.031353894621133804,
      "learning_rate": 4.2226148409894e-05,
      "loss": 0.0023,
      "step": 940
    },
    {
      "epoch": 2.8528528528528527,
      "grad_norm": 0.04232471436262131,
      "learning_rate": 4.204946996466431e-05,
      "loss": 0.0026,
      "step": 950
    },
    {
      "epoch": 2.8828828828828827,
      "grad_norm": 0.042371779680252075,
      "learning_rate": 4.1872791519434626e-05,
      "loss": 0.0025,
      "step": 960
    },
    {
      "epoch": 2.9129129129129128,
      "grad_norm": 0.0562010332942009,
      "learning_rate": 4.169611307420495e-05,
      "loss": 0.0023,
      "step": 970
    },
    {
      "epoch": 2.942942942942943,
      "grad_norm": 0.04467464238405228,
      "learning_rate": 4.1519434628975266e-05,
      "loss": 0.0023,
      "step": 980
    },
    {
      "epoch": 2.972972972972973,
      "grad_norm": 0.021933626383543015,
      "learning_rate": 4.134275618374558e-05,
      "loss": 0.0031,
      "step": 990
    },
    {
      "epoch": 3.003003003003003,
      "grad_norm": 0.04905001446604729,
      "learning_rate": 4.11660777385159e-05,
      "loss": 0.003,
      "step": 1000
    },
    {
      "epoch": 3.033033033033033,
      "grad_norm": 0.3255425691604614,
      "learning_rate": 4.0989399293286223e-05,
      "loss": 0.0027,
      "step": 1010
    },
    {
      "epoch": 3.063063063063063,
      "grad_norm": 0.02512349747121334,
      "learning_rate": 4.081272084805654e-05,
      "loss": 0.002,
      "step": 1020
    },
    {
      "epoch": 3.093093093093093,
      "grad_norm": 0.024630757048726082,
      "learning_rate": 4.063604240282686e-05,
      "loss": 0.0023,
      "step": 1030
    },
    {
      "epoch": 3.123123123123123,
      "grad_norm": 0.04368368908762932,
      "learning_rate": 4.0459363957597174e-05,
      "loss": 0.0026,
      "step": 1040
    },
    {
      "epoch": 3.153153153153153,
      "grad_norm": 0.03044859878718853,
      "learning_rate": 4.028268551236749e-05,
      "loss": 0.0022,
      "step": 1050
    },
    {
      "epoch": 3.1831831831831834,
      "grad_norm": 0.015517162159085274,
      "learning_rate": 4.0106007067137815e-05,
      "loss": 0.0019,
      "step": 1060
    },
    {
      "epoch": 3.2132132132132134,
      "grad_norm": 0.029799791052937508,
      "learning_rate": 3.992932862190813e-05,
      "loss": 0.0027,
      "step": 1070
    },
    {
      "epoch": 3.2432432432432434,
      "grad_norm": 0.030574021860957146,
      "learning_rate": 3.975265017667845e-05,
      "loss": 0.0022,
      "step": 1080
    },
    {
      "epoch": 3.2732732732732734,
      "grad_norm": 0.04931959882378578,
      "learning_rate": 3.9575971731448765e-05,
      "loss": 0.003,
      "step": 1090
    },
    {
      "epoch": 3.3033033033033035,
      "grad_norm": 0.03556809574365616,
      "learning_rate": 3.939929328621909e-05,
      "loss": 0.0026,
      "step": 1100
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.022808723151683807,
      "learning_rate": 3.9222614840989406e-05,
      "loss": 0.002,
      "step": 1110
    },
    {
      "epoch": 3.3633633633633635,
      "grad_norm": 0.033706579357385635,
      "learning_rate": 3.9045936395759716e-05,
      "loss": 0.0029,
      "step": 1120
    },
    {
      "epoch": 3.3933933933933935,
      "grad_norm": 0.032106127589941025,
      "learning_rate": 3.886925795053003e-05,
      "loss": 0.0023,
      "step": 1130
    },
    {
      "epoch": 3.4234234234234235,
      "grad_norm": 0.03797950595617294,
      "learning_rate": 3.869257950530035e-05,
      "loss": 0.0023,
      "step": 1140
    },
    {
      "epoch": 3.4534534534534536,
      "grad_norm": 0.030223360285162926,
      "learning_rate": 3.851590106007067e-05,
      "loss": 0.0026,
      "step": 1150
    },
    {
      "epoch": 3.4834834834834836,
      "grad_norm": 0.029825225472450256,
      "learning_rate": 3.833922261484099e-05,
      "loss": 0.0021,
      "step": 1160
    },
    {
      "epoch": 3.5135135135135136,
      "grad_norm": 0.0363008938729763,
      "learning_rate": 3.816254416961131e-05,
      "loss": 0.0022,
      "step": 1170
    },
    {
      "epoch": 3.5435435435435436,
      "grad_norm": 0.04002441465854645,
      "learning_rate": 3.7985865724381624e-05,
      "loss": 0.0018,
      "step": 1180
    },
    {
      "epoch": 3.5735735735735736,
      "grad_norm": 0.03677883371710777,
      "learning_rate": 3.780918727915195e-05,
      "loss": 0.002,
      "step": 1190
    },
    {
      "epoch": 3.6036036036036037,
      "grad_norm": 0.02736450545489788,
      "learning_rate": 3.7632508833922264e-05,
      "loss": 0.0025,
      "step": 1200
    },
    {
      "epoch": 3.6336336336336337,
      "grad_norm": 0.0369843915104866,
      "learning_rate": 3.745583038869258e-05,
      "loss": 0.0025,
      "step": 1210
    },
    {
      "epoch": 3.6636636636636637,
      "grad_norm": 0.036843057721853256,
      "learning_rate": 3.72791519434629e-05,
      "loss": 0.0026,
      "step": 1220
    },
    {
      "epoch": 3.6936936936936937,
      "grad_norm": 0.025216974318027496,
      "learning_rate": 3.710247349823322e-05,
      "loss": 0.0023,
      "step": 1230
    },
    {
      "epoch": 3.7237237237237237,
      "grad_norm": 0.017173513770103455,
      "learning_rate": 3.692579505300354e-05,
      "loss": 0.0019,
      "step": 1240
    },
    {
      "epoch": 3.7537537537537538,
      "grad_norm": 0.04303070530295372,
      "learning_rate": 3.6749116607773855e-05,
      "loss": 0.0027,
      "step": 1250
    },
    {
      "epoch": 3.7837837837837838,
      "grad_norm": 0.04049129784107208,
      "learning_rate": 3.657243816254417e-05,
      "loss": 0.0032,
      "step": 1260
    },
    {
      "epoch": 3.813813813813814,
      "grad_norm": 0.02441561222076416,
      "learning_rate": 3.639575971731449e-05,
      "loss": 0.0028,
      "step": 1270
    },
    {
      "epoch": 3.843843843843844,
      "grad_norm": 0.024590380489826202,
      "learning_rate": 3.621908127208481e-05,
      "loss": 0.0028,
      "step": 1280
    },
    {
      "epoch": 3.873873873873874,
      "grad_norm": 0.028117137029767036,
      "learning_rate": 3.604240282685513e-05,
      "loss": 0.0024,
      "step": 1290
    },
    {
      "epoch": 3.903903903903904,
      "grad_norm": 0.026523394510149956,
      "learning_rate": 3.586572438162544e-05,
      "loss": 0.0027,
      "step": 1300
    },
    {
      "epoch": 3.933933933933934,
      "grad_norm": 0.02950204163789749,
      "learning_rate": 3.5689045936395756e-05,
      "loss": 0.0031,
      "step": 1310
    },
    {
      "epoch": 3.963963963963964,
      "grad_norm": 0.02269328013062477,
      "learning_rate": 3.551236749116608e-05,
      "loss": 0.0026,
      "step": 1320
    },
    {
      "epoch": 3.993993993993994,
      "grad_norm": 0.03921385481953621,
      "learning_rate": 3.53356890459364e-05,
      "loss": 0.0026,
      "step": 1330
    },
    {
      "epoch": 4.024024024024024,
      "grad_norm": 0.1165524423122406,
      "learning_rate": 3.5159010600706714e-05,
      "loss": 0.0023,
      "step": 1340
    },
    {
      "epoch": 4.054054054054054,
      "grad_norm": 0.038583721965551376,
      "learning_rate": 3.498233215547703e-05,
      "loss": 0.0029,
      "step": 1350
    },
    {
      "epoch": 4.084084084084084,
      "grad_norm": 0.03275451809167862,
      "learning_rate": 3.480565371024735e-05,
      "loss": 0.0022,
      "step": 1360
    },
    {
      "epoch": 4.114114114114114,
      "grad_norm": 0.03919080272316933,
      "learning_rate": 3.462897526501767e-05,
      "loss": 0.0031,
      "step": 1370
    },
    {
      "epoch": 4.1441441441441444,
      "grad_norm": 0.03649377450346947,
      "learning_rate": 3.445229681978799e-05,
      "loss": 0.0027,
      "step": 1380
    },
    {
      "epoch": 4.1741741741741745,
      "grad_norm": 0.0303852129727602,
      "learning_rate": 3.4275618374558305e-05,
      "loss": 0.002,
      "step": 1390
    },
    {
      "epoch": 4.2042042042042045,
      "grad_norm": 0.05077548325061798,
      "learning_rate": 3.409893992932862e-05,
      "loss": 0.0028,
      "step": 1400
    },
    {
      "epoch": 4.2342342342342345,
      "grad_norm": 0.024918928742408752,
      "learning_rate": 3.3922261484098945e-05,
      "loss": 0.002,
      "step": 1410
    },
    {
      "epoch": 4.2642642642642645,
      "grad_norm": 0.031590785831213,
      "learning_rate": 3.374558303886926e-05,
      "loss": 0.002,
      "step": 1420
    },
    {
      "epoch": 4.2942942942942945,
      "grad_norm": 0.02976740524172783,
      "learning_rate": 3.356890459363958e-05,
      "loss": 0.0022,
      "step": 1430
    },
    {
      "epoch": 4.324324324324325,
      "grad_norm": 0.034524157643318176,
      "learning_rate": 3.3392226148409896e-05,
      "loss": 0.0022,
      "step": 1440
    },
    {
      "epoch": 4.354354354354355,
      "grad_norm": 0.038851696997880936,
      "learning_rate": 3.321554770318021e-05,
      "loss": 0.0027,
      "step": 1450
    },
    {
      "epoch": 4.384384384384385,
      "grad_norm": 0.04264548420906067,
      "learning_rate": 3.3038869257950536e-05,
      "loss": 0.0032,
      "step": 1460
    },
    {
      "epoch": 4.414414414414415,
      "grad_norm": 0.030639884993433952,
      "learning_rate": 3.286219081272085e-05,
      "loss": 0.0024,
      "step": 1470
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.017795806750655174,
      "learning_rate": 3.2685512367491163e-05,
      "loss": 0.002,
      "step": 1480
    },
    {
      "epoch": 4.474474474474475,
      "grad_norm": 0.027277113869786263,
      "learning_rate": 3.250883392226148e-05,
      "loss": 0.0021,
      "step": 1490
    },
    {
      "epoch": 4.504504504504505,
      "grad_norm": 0.07071398198604584,
      "learning_rate": 3.2332155477031804e-05,
      "loss": 0.0026,
      "step": 1500
    },
    {
      "epoch": 4.534534534534535,
      "grad_norm": 0.06328950077295303,
      "learning_rate": 3.215547703180212e-05,
      "loss": 0.0037,
      "step": 1510
    },
    {
      "epoch": 4.564564564564565,
      "grad_norm": 0.03521259129047394,
      "learning_rate": 3.197879858657244e-05,
      "loss": 0.0032,
      "step": 1520
    },
    {
      "epoch": 4.594594594594595,
      "grad_norm": 0.06561277061700821,
      "learning_rate": 3.1802120141342755e-05,
      "loss": 0.0031,
      "step": 1530
    },
    {
      "epoch": 4.624624624624625,
      "grad_norm": 0.1014142781496048,
      "learning_rate": 3.162544169611308e-05,
      "loss": 0.0029,
      "step": 1540
    },
    {
      "epoch": 4.654654654654655,
      "grad_norm": 0.03746899962425232,
      "learning_rate": 3.1448763250883395e-05,
      "loss": 0.0025,
      "step": 1550
    },
    {
      "epoch": 4.684684684684685,
      "grad_norm": 0.06459543108940125,
      "learning_rate": 3.127208480565371e-05,
      "loss": 0.0034,
      "step": 1560
    },
    {
      "epoch": 4.714714714714715,
      "grad_norm": 0.034904420375823975,
      "learning_rate": 3.109540636042403e-05,
      "loss": 0.0029,
      "step": 1570
    },
    {
      "epoch": 4.744744744744745,
      "grad_norm": 0.032444849610328674,
      "learning_rate": 3.0918727915194346e-05,
      "loss": 0.0028,
      "step": 1580
    },
    {
      "epoch": 4.774774774774775,
      "grad_norm": 0.042085420340299606,
      "learning_rate": 3.074204946996467e-05,
      "loss": 0.0033,
      "step": 1590
    },
    {
      "epoch": 4.804804804804805,
      "grad_norm": 0.035772912204265594,
      "learning_rate": 3.0565371024734986e-05,
      "loss": 0.0032,
      "step": 1600
    },
    {
      "epoch": 4.834834834834835,
      "grad_norm": 0.03243261203169823,
      "learning_rate": 3.03886925795053e-05,
      "loss": 0.003,
      "step": 1610
    },
    {
      "epoch": 4.864864864864865,
      "grad_norm": 0.03705514222383499,
      "learning_rate": 3.0212014134275616e-05,
      "loss": 0.0033,
      "step": 1620
    },
    {
      "epoch": 4.894894894894895,
      "grad_norm": 0.04704899713397026,
      "learning_rate": 3.003533568904594e-05,
      "loss": 0.0034,
      "step": 1630
    },
    {
      "epoch": 4.924924924924925,
      "grad_norm": 0.03603103756904602,
      "learning_rate": 2.9858657243816257e-05,
      "loss": 0.0031,
      "step": 1640
    },
    {
      "epoch": 4.954954954954955,
      "grad_norm": 0.03619915619492531,
      "learning_rate": 2.9681978798586574e-05,
      "loss": 0.0039,
      "step": 1650
    },
    {
      "epoch": 4.984984984984985,
      "grad_norm": 0.04140956327319145,
      "learning_rate": 2.950530035335689e-05,
      "loss": 0.0037,
      "step": 1660
    },
    {
      "epoch": 5.015015015015015,
      "grad_norm": 0.029309803619980812,
      "learning_rate": 2.9328621908127208e-05,
      "loss": 0.0032,
      "step": 1670
    },
    {
      "epoch": 5.045045045045045,
      "grad_norm": 0.03539786860346794,
      "learning_rate": 2.915194346289753e-05,
      "loss": 0.0029,
      "step": 1680
    },
    {
      "epoch": 5.075075075075075,
      "grad_norm": 0.0484224334359169,
      "learning_rate": 2.8975265017667848e-05,
      "loss": 0.0028,
      "step": 1690
    },
    {
      "epoch": 5.105105105105105,
      "grad_norm": 0.0645689144730568,
      "learning_rate": 2.879858657243816e-05,
      "loss": 0.0036,
      "step": 1700
    },
    {
      "epoch": 5.135135135135135,
      "grad_norm": 0.03729238361120224,
      "learning_rate": 2.862190812720848e-05,
      "loss": 0.0027,
      "step": 1710
    },
    {
      "epoch": 5.165165165165165,
      "grad_norm": 0.036596138030290604,
      "learning_rate": 2.8445229681978802e-05,
      "loss": 0.0033,
      "step": 1720
    },
    {
      "epoch": 5.195195195195195,
      "grad_norm": 0.055986400693655014,
      "learning_rate": 2.826855123674912e-05,
      "loss": 0.0024,
      "step": 1730
    },
    {
      "epoch": 5.225225225225225,
      "grad_norm": 0.051108211278915405,
      "learning_rate": 2.8091872791519436e-05,
      "loss": 0.0035,
      "step": 1740
    },
    {
      "epoch": 5.255255255255255,
      "grad_norm": 0.04740056395530701,
      "learning_rate": 2.7915194346289753e-05,
      "loss": 0.004,
      "step": 1750
    },
    {
      "epoch": 5.285285285285285,
      "grad_norm": 0.04495342820882797,
      "learning_rate": 2.773851590106007e-05,
      "loss": 0.003,
      "step": 1760
    },
    {
      "epoch": 5.315315315315315,
      "grad_norm": 0.03190266713500023,
      "learning_rate": 2.7561837455830393e-05,
      "loss": 0.0021,
      "step": 1770
    },
    {
      "epoch": 5.345345345345345,
      "grad_norm": 0.06070832163095474,
      "learning_rate": 2.738515901060071e-05,
      "loss": 0.0034,
      "step": 1780
    },
    {
      "epoch": 5.375375375375375,
      "grad_norm": 0.03665340319275856,
      "learning_rate": 2.7208480565371023e-05,
      "loss": 0.0036,
      "step": 1790
    },
    {
      "epoch": 5.405405405405405,
      "grad_norm": 0.032549768686294556,
      "learning_rate": 2.703180212014134e-05,
      "loss": 0.0034,
      "step": 1800
    },
    {
      "epoch": 5.435435435435435,
      "grad_norm": 0.03470896929502487,
      "learning_rate": 2.6855123674911664e-05,
      "loss": 0.0025,
      "step": 1810
    },
    {
      "epoch": 5.465465465465465,
      "grad_norm": 0.03540385141968727,
      "learning_rate": 2.667844522968198e-05,
      "loss": 0.0029,
      "step": 1820
    },
    {
      "epoch": 5.495495495495495,
      "grad_norm": 0.03921915590763092,
      "learning_rate": 2.6501766784452298e-05,
      "loss": 0.0036,
      "step": 1830
    },
    {
      "epoch": 5.525525525525525,
      "grad_norm": 0.03666319698095322,
      "learning_rate": 2.6325088339222615e-05,
      "loss": 0.0038,
      "step": 1840
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 0.05582333356142044,
      "learning_rate": 2.6148409893992938e-05,
      "loss": 0.0029,
      "step": 1850
    },
    {
      "epoch": 5.585585585585585,
      "grad_norm": 0.03694199025630951,
      "learning_rate": 2.5971731448763255e-05,
      "loss": 0.0028,
      "step": 1860
    },
    {
      "epoch": 5.615615615615615,
      "grad_norm": 0.055830106139183044,
      "learning_rate": 2.5795053003533572e-05,
      "loss": 0.0024,
      "step": 1870
    },
    {
      "epoch": 5.645645645645645,
      "grad_norm": 0.04204340651631355,
      "learning_rate": 2.5618374558303885e-05,
      "loss": 0.0026,
      "step": 1880
    },
    {
      "epoch": 5.675675675675675,
      "grad_norm": 0.03769915550947189,
      "learning_rate": 2.5441696113074202e-05,
      "loss": 0.0032,
      "step": 1890
    },
    {
      "epoch": 5.7057057057057055,
      "grad_norm": 0.05640937387943268,
      "learning_rate": 2.5265017667844526e-05,
      "loss": 0.0032,
      "step": 1900
    },
    {
      "epoch": 5.7357357357357355,
      "grad_norm": 0.047805577516555786,
      "learning_rate": 2.5088339222614843e-05,
      "loss": 0.0033,
      "step": 1910
    },
    {
      "epoch": 5.7657657657657655,
      "grad_norm": 0.046602796763181686,
      "learning_rate": 2.491166077738516e-05,
      "loss": 0.0036,
      "step": 1920
    },
    {
      "epoch": 5.7957957957957955,
      "grad_norm": 0.048017147928476334,
      "learning_rate": 2.473498233215548e-05,
      "loss": 0.0038,
      "step": 1930
    },
    {
      "epoch": 5.8258258258258255,
      "grad_norm": 0.03575769439339638,
      "learning_rate": 2.4558303886925797e-05,
      "loss": 0.0037,
      "step": 1940
    },
    {
      "epoch": 5.8558558558558556,
      "grad_norm": 0.07485134154558182,
      "learning_rate": 2.4381625441696117e-05,
      "loss": 0.0038,
      "step": 1950
    },
    {
      "epoch": 5.885885885885886,
      "grad_norm": 0.04128864407539368,
      "learning_rate": 2.420494699646643e-05,
      "loss": 0.0033,
      "step": 1960
    },
    {
      "epoch": 5.915915915915916,
      "grad_norm": 0.035099904984235764,
      "learning_rate": 2.4028268551236747e-05,
      "loss": 0.0033,
      "step": 1970
    },
    {
      "epoch": 5.945945945945946,
      "grad_norm": 0.05051794648170471,
      "learning_rate": 2.3851590106007068e-05,
      "loss": 0.0041,
      "step": 1980
    },
    {
      "epoch": 5.975975975975976,
      "grad_norm": 0.03849436715245247,
      "learning_rate": 2.3674911660777384e-05,
      "loss": 0.0041,
      "step": 1990
    },
    {
      "epoch": 6.006006006006006,
      "grad_norm": 0.0923493430018425,
      "learning_rate": 2.3498233215547705e-05,
      "loss": 0.0041,
      "step": 2000
    },
    {
      "epoch": 6.036036036036036,
      "grad_norm": 0.05175864323973656,
      "learning_rate": 2.332155477031802e-05,
      "loss": 0.0028,
      "step": 2010
    },
    {
      "epoch": 6.066066066066066,
      "grad_norm": 0.03563743829727173,
      "learning_rate": 2.3144876325088342e-05,
      "loss": 0.003,
      "step": 2020
    },
    {
      "epoch": 6.096096096096096,
      "grad_norm": 0.08746835589408875,
      "learning_rate": 2.296819787985866e-05,
      "loss": 0.0042,
      "step": 2030
    },
    {
      "epoch": 6.126126126126126,
      "grad_norm": 0.03772304207086563,
      "learning_rate": 2.279151943462898e-05,
      "loss": 0.0041,
      "step": 2040
    },
    {
      "epoch": 6.156156156156156,
      "grad_norm": 0.03519727662205696,
      "learning_rate": 2.2614840989399292e-05,
      "loss": 0.0034,
      "step": 2050
    },
    {
      "epoch": 6.186186186186186,
      "grad_norm": 0.044658683240413666,
      "learning_rate": 2.2438162544169613e-05,
      "loss": 0.0029,
      "step": 2060
    },
    {
      "epoch": 6.216216216216216,
      "grad_norm": 0.04464785382151604,
      "learning_rate": 2.226148409893993e-05,
      "loss": 0.0033,
      "step": 2070
    },
    {
      "epoch": 6.246246246246246,
      "grad_norm": 0.049312006682157516,
      "learning_rate": 2.2084805653710246e-05,
      "loss": 0.0036,
      "step": 2080
    },
    {
      "epoch": 6.276276276276276,
      "grad_norm": 0.052271898835897446,
      "learning_rate": 2.1908127208480567e-05,
      "loss": 0.0034,
      "step": 2090
    },
    {
      "epoch": 6.306306306306306,
      "grad_norm": 0.06338706612586975,
      "learning_rate": 2.1731448763250883e-05,
      "loss": 0.0035,
      "step": 2100
    },
    {
      "epoch": 6.336336336336337,
      "grad_norm": 0.053343139588832855,
      "learning_rate": 2.1554770318021204e-05,
      "loss": 0.0039,
      "step": 2110
    },
    {
      "epoch": 6.366366366366367,
      "grad_norm": 0.05164261907339096,
      "learning_rate": 2.137809187279152e-05,
      "loss": 0.003,
      "step": 2120
    },
    {
      "epoch": 6.396396396396397,
      "grad_norm": 0.10711455345153809,
      "learning_rate": 2.120141342756184e-05,
      "loss": 0.0045,
      "step": 2130
    },
    {
      "epoch": 6.426426426426427,
      "grad_norm": 0.206574946641922,
      "learning_rate": 2.1024734982332154e-05,
      "loss": 0.0039,
      "step": 2140
    },
    {
      "epoch": 6.456456456456457,
      "grad_norm": 0.05465932935476303,
      "learning_rate": 2.0848056537102475e-05,
      "loss": 0.0039,
      "step": 2150
    },
    {
      "epoch": 6.486486486486487,
      "grad_norm": 0.06044886261224747,
      "learning_rate": 2.067137809187279e-05,
      "loss": 0.004,
      "step": 2160
    },
    {
      "epoch": 6.516516516516517,
      "grad_norm": 0.0691816434264183,
      "learning_rate": 2.0494699646643112e-05,
      "loss": 0.0045,
      "step": 2170
    },
    {
      "epoch": 6.546546546546547,
      "grad_norm": 0.0489208959043026,
      "learning_rate": 2.031802120141343e-05,
      "loss": 0.0038,
      "step": 2180
    },
    {
      "epoch": 6.576576576576577,
      "grad_norm": 0.050344064831733704,
      "learning_rate": 2.0141342756183745e-05,
      "loss": 0.004,
      "step": 2190
    },
    {
      "epoch": 6.606606606606607,
      "grad_norm": 0.09807814657688141,
      "learning_rate": 1.9964664310954066e-05,
      "loss": 0.004,
      "step": 2200
    },
    {
      "epoch": 6.636636636636637,
      "grad_norm": 0.03831368684768677,
      "learning_rate": 1.9787985865724383e-05,
      "loss": 0.0036,
      "step": 2210
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.047072283923625946,
      "learning_rate": 1.9611307420494703e-05,
      "loss": 0.0036,
      "step": 2220
    },
    {
      "epoch": 6.696696696696697,
      "grad_norm": 0.07596630603075027,
      "learning_rate": 1.9434628975265016e-05,
      "loss": 0.0051,
      "step": 2230
    },
    {
      "epoch": 6.726726726726727,
      "grad_norm": 0.051230691373348236,
      "learning_rate": 1.9257950530035337e-05,
      "loss": 0.0039,
      "step": 2240
    },
    {
      "epoch": 6.756756756756757,
      "grad_norm": 0.07653719931840897,
      "learning_rate": 1.9081272084805653e-05,
      "loss": 0.004,
      "step": 2250
    },
    {
      "epoch": 6.786786786786787,
      "grad_norm": 0.0651228278875351,
      "learning_rate": 1.8904593639575974e-05,
      "loss": 0.0046,
      "step": 2260
    },
    {
      "epoch": 6.816816816816817,
      "grad_norm": 0.05030855908989906,
      "learning_rate": 1.872791519434629e-05,
      "loss": 0.0041,
      "step": 2270
    },
    {
      "epoch": 6.846846846846847,
      "grad_norm": 0.043201133608818054,
      "learning_rate": 1.855123674911661e-05,
      "loss": 0.0046,
      "step": 2280
    },
    {
      "epoch": 6.876876876876877,
      "grad_norm": 0.054833002388477325,
      "learning_rate": 1.8374558303886928e-05,
      "loss": 0.0039,
      "step": 2290
    },
    {
      "epoch": 6.906906906906907,
      "grad_norm": 0.0665232464671135,
      "learning_rate": 1.8197879858657244e-05,
      "loss": 0.0039,
      "step": 2300
    },
    {
      "epoch": 6.936936936936937,
      "grad_norm": 0.07868718355894089,
      "learning_rate": 1.8021201413427565e-05,
      "loss": 0.0041,
      "step": 2310
    },
    {
      "epoch": 6.966966966966967,
      "grad_norm": 0.04498507082462311,
      "learning_rate": 1.7844522968197878e-05,
      "loss": 0.0041,
      "step": 2320
    },
    {
      "epoch": 6.996996996996997,
      "grad_norm": 0.057257384061813354,
      "learning_rate": 1.76678445229682e-05,
      "loss": 0.0048,
      "step": 2330
    },
    {
      "epoch": 7.027027027027027,
      "grad_norm": 0.08898471295833588,
      "learning_rate": 1.7491166077738515e-05,
      "loss": 0.0041,
      "step": 2340
    },
    {
      "epoch": 7.057057057057057,
      "grad_norm": 0.06582199037075043,
      "learning_rate": 1.7314487632508836e-05,
      "loss": 0.0039,
      "step": 2350
    },
    {
      "epoch": 7.087087087087087,
      "grad_norm": 0.07757893949747086,
      "learning_rate": 1.7137809187279152e-05,
      "loss": 0.0038,
      "step": 2360
    },
    {
      "epoch": 7.117117117117117,
      "grad_norm": 0.06318993866443634,
      "learning_rate": 1.6961130742049473e-05,
      "loss": 0.0049,
      "step": 2370
    },
    {
      "epoch": 7.147147147147147,
      "grad_norm": 0.1374209076166153,
      "learning_rate": 1.678445229681979e-05,
      "loss": 0.0052,
      "step": 2380
    },
    {
      "epoch": 7.177177177177177,
      "grad_norm": 0.034742321819067,
      "learning_rate": 1.6607773851590106e-05,
      "loss": 0.0041,
      "step": 2390
    },
    {
      "epoch": 7.207207207207207,
      "grad_norm": 0.07932966947555542,
      "learning_rate": 1.6431095406360427e-05,
      "loss": 0.0043,
      "step": 2400
    },
    {
      "epoch": 7.237237237237237,
      "grad_norm": 0.060195304453372955,
      "learning_rate": 1.625441696113074e-05,
      "loss": 0.0051,
      "step": 2410
    },
    {
      "epoch": 7.267267267267267,
      "grad_norm": 0.03757830336689949,
      "learning_rate": 1.607773851590106e-05,
      "loss": 0.0049,
      "step": 2420
    },
    {
      "epoch": 7.297297297297297,
      "grad_norm": 0.04161221906542778,
      "learning_rate": 1.5901060070671377e-05,
      "loss": 0.0049,
      "step": 2430
    },
    {
      "epoch": 7.327327327327327,
      "grad_norm": 0.139322429895401,
      "learning_rate": 1.5724381625441698e-05,
      "loss": 0.0046,
      "step": 2440
    },
    {
      "epoch": 7.357357357357357,
      "grad_norm": 0.09959810227155685,
      "learning_rate": 1.5547703180212014e-05,
      "loss": 0.0061,
      "step": 2450
    },
    {
      "epoch": 7.387387387387387,
      "grad_norm": 0.10860658437013626,
      "learning_rate": 1.5371024734982335e-05,
      "loss": 0.0064,
      "step": 2460
    },
    {
      "epoch": 7.4174174174174174,
      "grad_norm": 0.12046618014574051,
      "learning_rate": 1.519434628975265e-05,
      "loss": 0.0042,
      "step": 2470
    },
    {
      "epoch": 7.4474474474474475,
      "grad_norm": 0.06600851565599442,
      "learning_rate": 1.501766784452297e-05,
      "loss": 0.0053,
      "step": 2480
    },
    {
      "epoch": 7.4774774774774775,
      "grad_norm": 0.051929812878370285,
      "learning_rate": 1.4840989399293287e-05,
      "loss": 0.0053,
      "step": 2490
    },
    {
      "epoch": 7.5075075075075075,
      "grad_norm": 0.07947667688131332,
      "learning_rate": 1.4664310954063604e-05,
      "loss": 0.0064,
      "step": 2500
    },
    {
      "epoch": 7.5375375375375375,
      "grad_norm": 0.058032114058732986,
      "learning_rate": 1.4487632508833924e-05,
      "loss": 0.0046,
      "step": 2510
    },
    {
      "epoch": 7.5675675675675675,
      "grad_norm": 0.05267517268657684,
      "learning_rate": 1.431095406360424e-05,
      "loss": 0.0058,
      "step": 2520
    },
    {
      "epoch": 7.597597597597598,
      "grad_norm": 0.07649222016334534,
      "learning_rate": 1.413427561837456e-05,
      "loss": 0.0061,
      "step": 2530
    },
    {
      "epoch": 7.627627627627628,
      "grad_norm": 0.042496196925640106,
      "learning_rate": 1.3957597173144876e-05,
      "loss": 0.005,
      "step": 2540
    },
    {
      "epoch": 7.657657657657658,
      "grad_norm": 0.06470546871423721,
      "learning_rate": 1.3780918727915197e-05,
      "loss": 0.0037,
      "step": 2550
    },
    {
      "epoch": 7.687687687687688,
      "grad_norm": 0.059701405465602875,
      "learning_rate": 1.3604240282685512e-05,
      "loss": 0.0052,
      "step": 2560
    },
    {
      "epoch": 7.717717717717718,
      "grad_norm": 0.08756651729345322,
      "learning_rate": 1.3427561837455832e-05,
      "loss": 0.0058,
      "step": 2570
    },
    {
      "epoch": 7.747747747747748,
      "grad_norm": 0.05850404500961304,
      "learning_rate": 1.3250883392226149e-05,
      "loss": 0.0048,
      "step": 2580
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 0.11118514835834503,
      "learning_rate": 1.3074204946996469e-05,
      "loss": 0.0073,
      "step": 2590
    },
    {
      "epoch": 7.807807807807808,
      "grad_norm": 0.053099606186151505,
      "learning_rate": 1.2897526501766786e-05,
      "loss": 0.005,
      "step": 2600
    },
    {
      "epoch": 7.837837837837838,
      "grad_norm": 0.0717935562133789,
      "learning_rate": 1.2720848056537101e-05,
      "loss": 0.0065,
      "step": 2610
    },
    {
      "epoch": 7.867867867867868,
      "grad_norm": 0.07322903722524643,
      "learning_rate": 1.2544169611307421e-05,
      "loss": 0.0067,
      "step": 2620
    },
    {
      "epoch": 7.897897897897898,
      "grad_norm": 0.09497884660959244,
      "learning_rate": 1.236749116607774e-05,
      "loss": 0.0079,
      "step": 2630
    },
    {
      "epoch": 7.927927927927928,
      "grad_norm": 0.0670909583568573,
      "learning_rate": 1.2190812720848058e-05,
      "loss": 0.0056,
      "step": 2640
    },
    {
      "epoch": 7.957957957957958,
      "grad_norm": 0.09551277756690979,
      "learning_rate": 1.2014134275618374e-05,
      "loss": 0.0067,
      "step": 2650
    },
    {
      "epoch": 7.987987987987988,
      "grad_norm": 0.05661780387163162,
      "learning_rate": 1.1837455830388692e-05,
      "loss": 0.0059,
      "step": 2660
    },
    {
      "epoch": 8.018018018018019,
      "grad_norm": 0.07795168459415436,
      "learning_rate": 1.166077738515901e-05,
      "loss": 0.0049,
      "step": 2670
    },
    {
      "epoch": 8.048048048048049,
      "grad_norm": 0.07769442349672318,
      "learning_rate": 1.148409893992933e-05,
      "loss": 0.0061,
      "step": 2680
    },
    {
      "epoch": 8.078078078078079,
      "grad_norm": 0.2815549671649933,
      "learning_rate": 1.1307420494699646e-05,
      "loss": 0.0069,
      "step": 2690
    },
    {
      "epoch": 8.108108108108109,
      "grad_norm": 0.1154598593711853,
      "learning_rate": 1.1130742049469965e-05,
      "loss": 0.0064,
      "step": 2700
    },
    {
      "epoch": 8.138138138138139,
      "grad_norm": 0.09219738841056824,
      "learning_rate": 1.0954063604240283e-05,
      "loss": 0.0062,
      "step": 2710
    },
    {
      "epoch": 8.168168168168169,
      "grad_norm": 0.07860469073057175,
      "learning_rate": 1.0777385159010602e-05,
      "loss": 0.0063,
      "step": 2720
    },
    {
      "epoch": 8.198198198198199,
      "grad_norm": 0.09649043530225754,
      "learning_rate": 1.060070671378092e-05,
      "loss": 0.0074,
      "step": 2730
    },
    {
      "epoch": 8.228228228228229,
      "grad_norm": 0.10646608471870422,
      "learning_rate": 1.0424028268551237e-05,
      "loss": 0.0061,
      "step": 2740
    },
    {
      "epoch": 8.258258258258259,
      "grad_norm": 0.04795586317777634,
      "learning_rate": 1.0247349823321556e-05,
      "loss": 0.0073,
      "step": 2750
    },
    {
      "epoch": 8.288288288288289,
      "grad_norm": 0.11194753646850586,
      "learning_rate": 1.0070671378091873e-05,
      "loss": 0.0061,
      "step": 2760
    },
    {
      "epoch": 8.318318318318319,
      "grad_norm": 0.11233054101467133,
      "learning_rate": 9.893992932862191e-06,
      "loss": 0.0059,
      "step": 2770
    },
    {
      "epoch": 8.348348348348349,
      "grad_norm": 0.08699765801429749,
      "learning_rate": 9.717314487632508e-06,
      "loss": 0.0063,
      "step": 2780
    },
    {
      "epoch": 8.378378378378379,
      "grad_norm": 0.13595175743103027,
      "learning_rate": 9.540636042402827e-06,
      "loss": 0.0103,
      "step": 2790
    },
    {
      "epoch": 8.408408408408409,
      "grad_norm": 0.29727858304977417,
      "learning_rate": 9.363957597173145e-06,
      "loss": 0.0092,
      "step": 2800
    },
    {
      "epoch": 8.438438438438439,
      "grad_norm": 0.11542025953531265,
      "learning_rate": 9.187279151943464e-06,
      "loss": 0.0058,
      "step": 2810
    },
    {
      "epoch": 8.468468468468469,
      "grad_norm": 0.16483190655708313,
      "learning_rate": 9.010600706713782e-06,
      "loss": 0.0088,
      "step": 2820
    },
    {
      "epoch": 8.498498498498499,
      "grad_norm": 0.261989027261734,
      "learning_rate": 8.8339222614841e-06,
      "loss": 0.0068,
      "step": 2830
    },
    {
      "epoch": 8.528528528528529,
      "grad_norm": 0.11104105412960052,
      "learning_rate": 8.657243816254418e-06,
      "loss": 0.0086,
      "step": 2840
    },
    {
      "epoch": 8.558558558558559,
      "grad_norm": 0.1544448286294937,
      "learning_rate": 8.480565371024736e-06,
      "loss": 0.0115,
      "step": 2850
    },
    {
      "epoch": 8.588588588588589,
      "grad_norm": 0.08225113153457642,
      "learning_rate": 8.303886925795053e-06,
      "loss": 0.0083,
      "step": 2860
    },
    {
      "epoch": 8.618618618618619,
      "grad_norm": 0.12394561618566513,
      "learning_rate": 8.12720848056537e-06,
      "loss": 0.0086,
      "step": 2870
    },
    {
      "epoch": 8.64864864864865,
      "grad_norm": 0.107894666492939,
      "learning_rate": 7.950530035335689e-06,
      "loss": 0.0087,
      "step": 2880
    },
    {
      "epoch": 8.67867867867868,
      "grad_norm": 0.1002020388841629,
      "learning_rate": 7.773851590106007e-06,
      "loss": 0.0083,
      "step": 2890
    },
    {
      "epoch": 8.70870870870871,
      "grad_norm": 0.10844592750072479,
      "learning_rate": 7.597173144876325e-06,
      "loss": 0.0075,
      "step": 2900
    },
    {
      "epoch": 8.73873873873874,
      "grad_norm": 0.13182714581489563,
      "learning_rate": 7.4204946996466435e-06,
      "loss": 0.008,
      "step": 2910
    },
    {
      "epoch": 8.76876876876877,
      "grad_norm": 0.13451780378818512,
      "learning_rate": 7.243816254416962e-06,
      "loss": 0.0092,
      "step": 2920
    },
    {
      "epoch": 8.7987987987988,
      "grad_norm": 0.12298419326543808,
      "learning_rate": 7.06713780918728e-06,
      "loss": 0.0087,
      "step": 2930
    },
    {
      "epoch": 8.82882882882883,
      "grad_norm": 0.13147498667240143,
      "learning_rate": 6.890459363957598e-06,
      "loss": 0.0079,
      "step": 2940
    },
    {
      "epoch": 8.85885885885886,
      "grad_norm": 0.09073835611343384,
      "learning_rate": 6.713780918727916e-06,
      "loss": 0.0083,
      "step": 2950
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.11852949857711792,
      "learning_rate": 6.5371024734982345e-06,
      "loss": 0.0118,
      "step": 2960
    },
    {
      "epoch": 8.91891891891892,
      "grad_norm": 0.08586928993463516,
      "learning_rate": 6.3604240282685506e-06,
      "loss": 0.009,
      "step": 2970
    },
    {
      "epoch": 8.94894894894895,
      "grad_norm": 0.15969087183475494,
      "learning_rate": 6.18374558303887e-06,
      "loss": 0.0094,
      "step": 2980
    },
    {
      "epoch": 8.97897897897898,
      "grad_norm": 0.11431818455457687,
      "learning_rate": 6.007067137809187e-06,
      "loss": 0.0099,
      "step": 2990
    },
    {
      "epoch": 9.00900900900901,
      "grad_norm": 0.19209367036819458,
      "learning_rate": 5.830388692579505e-06,
      "loss": 0.0097,
      "step": 3000
    },
    {
      "epoch": 9.03903903903904,
      "grad_norm": 0.09138575196266174,
      "learning_rate": 5.653710247349823e-06,
      "loss": 0.009,
      "step": 3010
    },
    {
      "epoch": 9.06906906906907,
      "grad_norm": 0.09252845495939255,
      "learning_rate": 5.477031802120142e-06,
      "loss": 0.0101,
      "step": 3020
    },
    {
      "epoch": 9.0990990990991,
      "grad_norm": 0.1038074940443039,
      "learning_rate": 5.30035335689046e-06,
      "loss": 0.0094,
      "step": 3030
    },
    {
      "epoch": 9.12912912912913,
      "grad_norm": 0.08215226233005524,
      "learning_rate": 5.123674911660778e-06,
      "loss": 0.0087,
      "step": 3040
    },
    {
      "epoch": 9.15915915915916,
      "grad_norm": 0.1916288584470749,
      "learning_rate": 4.946996466431096e-06,
      "loss": 0.0082,
      "step": 3050
    },
    {
      "epoch": 9.18918918918919,
      "grad_norm": 0.10187273472547531,
      "learning_rate": 4.770318021201413e-06,
      "loss": 0.0114,
      "step": 3060
    },
    {
      "epoch": 9.21921921921922,
      "grad_norm": 0.08554563671350479,
      "learning_rate": 4.593639575971732e-06,
      "loss": 0.0084,
      "step": 3070
    },
    {
      "epoch": 9.24924924924925,
      "grad_norm": 0.08797533810138702,
      "learning_rate": 4.41696113074205e-06,
      "loss": 0.0094,
      "step": 3080
    },
    {
      "epoch": 9.27927927927928,
      "grad_norm": 0.15360139310359955,
      "learning_rate": 4.240282685512368e-06,
      "loss": 0.0128,
      "step": 3090
    },
    {
      "epoch": 9.30930930930931,
      "grad_norm": 0.12858286499977112,
      "learning_rate": 4.063604240282685e-06,
      "loss": 0.0128,
      "step": 3100
    },
    {
      "epoch": 9.33933933933934,
      "grad_norm": 0.17842251062393188,
      "learning_rate": 3.886925795053004e-06,
      "loss": 0.0129,
      "step": 3110
    },
    {
      "epoch": 9.36936936936937,
      "grad_norm": 0.15891583263874054,
      "learning_rate": 3.7102473498233217e-06,
      "loss": 0.0111,
      "step": 3120
    },
    {
      "epoch": 9.3993993993994,
      "grad_norm": 0.2899267375469208,
      "learning_rate": 3.53356890459364e-06,
      "loss": 0.0161,
      "step": 3130
    },
    {
      "epoch": 9.42942942942943,
      "grad_norm": 0.17095914483070374,
      "learning_rate": 3.356890459363958e-06,
      "loss": 0.0131,
      "step": 3140
    },
    {
      "epoch": 9.45945945945946,
      "grad_norm": 0.13392147421836853,
      "learning_rate": 3.1802120141342753e-06,
      "loss": 0.0139,
      "step": 3150
    },
    {
      "epoch": 9.48948948948949,
      "grad_norm": 0.3177904486656189,
      "learning_rate": 3.0035335689045934e-06,
      "loss": 0.0144,
      "step": 3160
    },
    {
      "epoch": 9.51951951951952,
      "grad_norm": 0.06216108426451683,
      "learning_rate": 2.8268551236749116e-06,
      "loss": 0.0141,
      "step": 3170
    },
    {
      "epoch": 9.54954954954955,
      "grad_norm": 0.21049804985523224,
      "learning_rate": 2.65017667844523e-06,
      "loss": 0.0145,
      "step": 3180
    },
    {
      "epoch": 9.57957957957958,
      "grad_norm": 0.13760194182395935,
      "learning_rate": 2.473498233215548e-06,
      "loss": 0.0124,
      "step": 3190
    },
    {
      "epoch": 9.60960960960961,
      "grad_norm": 0.23089905083179474,
      "learning_rate": 2.296819787985866e-06,
      "loss": 0.0138,
      "step": 3200
    },
    {
      "epoch": 9.63963963963964,
      "grad_norm": 0.2178541123867035,
      "learning_rate": 2.120141342756184e-06,
      "loss": 0.0153,
      "step": 3210
    },
    {
      "epoch": 9.66966966966967,
      "grad_norm": 0.23992204666137695,
      "learning_rate": 1.943462897526502e-06,
      "loss": 0.0155,
      "step": 3220
    },
    {
      "epoch": 9.6996996996997,
      "grad_norm": 0.18068604171276093,
      "learning_rate": 1.76678445229682e-06,
      "loss": 0.0124,
      "step": 3230
    },
    {
      "epoch": 9.72972972972973,
      "grad_norm": 0.23046624660491943,
      "learning_rate": 1.5901060070671376e-06,
      "loss": 0.0192,
      "step": 3240
    },
    {
      "epoch": 9.75975975975976,
      "grad_norm": 0.2662811279296875,
      "learning_rate": 1.4134275618374558e-06,
      "loss": 0.0189,
      "step": 3250
    },
    {
      "epoch": 9.78978978978979,
      "grad_norm": 0.3317203223705292,
      "learning_rate": 1.236749116607774e-06,
      "loss": 0.0134,
      "step": 3260
    },
    {
      "epoch": 9.81981981981982,
      "grad_norm": 0.19849663972854614,
      "learning_rate": 1.060070671378092e-06,
      "loss": 0.0153,
      "step": 3270
    },
    {
      "epoch": 9.84984984984985,
      "grad_norm": 0.26300954818725586,
      "learning_rate": 8.8339222614841e-07,
      "loss": 0.023,
      "step": 3280
    },
    {
      "epoch": 9.87987987987988,
      "grad_norm": 0.3597000241279602,
      "learning_rate": 7.067137809187279e-07,
      "loss": 0.0189,
      "step": 3290
    },
    {
      "epoch": 9.90990990990991,
      "grad_norm": 0.16424889862537384,
      "learning_rate": 5.30035335689046e-07,
      "loss": 0.0173,
      "step": 3300
    },
    {
      "epoch": 9.93993993993994,
      "grad_norm": 0.34489744901657104,
      "learning_rate": 3.5335689045936394e-07,
      "loss": 0.0157,
      "step": 3310
    },
    {
      "epoch": 9.96996996996997,
      "grad_norm": 0.22962883114814758,
      "learning_rate": 1.7667844522968197e-07,
      "loss": 0.0274,
      "step": 3320
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.3395178020000458,
      "learning_rate": 0.0,
      "loss": 0.032,
      "step": 3330
    },
    {
      "epoch": 10.0,
      "step": 3330,
      "total_flos": 3605505591214080.0,
      "train_loss": 0.005488963807197484,
      "train_runtime": 1766.8001,
      "train_samples_per_second": 15.078,
      "train_steps_per_second": 1.885
    }
  ],
  "logging_steps": 10,
  "max_steps": 3330,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3605505591214080.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
